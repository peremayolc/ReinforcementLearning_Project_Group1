# -*- coding: utf-8 -*-
"""KABOOM REINFORCE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VH9v8xrDuhdTQVOi1rqnelKKBQ9og2zi
"""
import gymnasium as gym

import ale_py
from ale_py import ALEInterface
from ale_py import ALEState


#import evenrything needed
import torch
import torch.nn as nn
import torch.optim as optim
from torchsummary import summary

import collections

import datetime
import time
import numpy as np

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np

import os

#check if gpu is available.
if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
#get name of device being used.
print(device)

import warnings
from typing import Any, ClassVar, Optional, TypeVar, Union, Type, Dict

import numpy as np
import torch as th
from gymnasium import spaces
from torch.nn import functional as F

from stable_baselines3.common.buffers import RolloutBuffer
from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm
from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy
from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule
from stable_baselines3.common.utils import explained_variance, get_schedule_fn
from stable_baselines3 import PPO


from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3.common.callbacks import BaseCallback


#generate environment
env = make_atari_env("Assault-v4", n_envs=4, seed=0)
env = VecFrameStack(env, n_stack=4)

#to get results at each iteration
class RewardTrackingCallback(BaseCallback):
    def __init__(self, rolling_window=10, verbose=0):
        super(RewardTrackingCallback, self).__init__(verbose)
        self.rolling_window = rolling_window
        self.episode_rewards = []
        self.rolling_avg_rewards = []
        self.loss_values = []

    def _on_step(self) -> bool:
        # Track episode rewards
        if self.locals.get("infos"):
            for info in self.locals["infos"]:
                if "episode" in info.keys():
                    self.episode_rewards.append(info["episode"]["r"])
                    # Calculate rolling average
                    if len(self.episode_rewards) >= self.rolling_window:
                        avg_reward = np.mean(self.episode_rewards[-self.rolling_window:])
                        self.rolling_avg_rewards.append(avg_reward)
                    else:
                        self.rolling_avg_rewards.append(np.mean(self.episode_rewards))

        # Track loss
        if "loss" in self.locals:
            self.loss_values.append(self.locals["loss"])

        return True

#generate callback to log results
callback = RewardTrackingCallback(rolling_window=10)

n_steps_values = [2048, 6144]
learning_rates = [0.0003, 2.5e-5, 1e-5]

for lr in learning_rates:
    for n_step in n_steps_values:
        model = PPO("CnnPolicy", env, verbose=1, learning_rate=lr, n_steps=n_step)
        model.learn(total_timesteps=1_000_000, callback=callback)

        model.save(f"ASSAULT_PPO{lr}{n_step}")


        plt.figure(figsize=(16, 10))
        plt.subplot(1, 3, 1)
        plt.plot(callback.episode_rewards, label='Episode Reward')
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.title('Reward Evolution')
        plt.legend()

        plt.subplot(1, 3, 2)
        plt.plot(callback.rolling_avg_rewards, label='Rolling Average Reward (10 episodes)')
        plt.xlabel('Episode')
        plt.ylabel('Average Reward')
        plt.title('Rolling Average Reward')
        plt.legend()

        plt.subplot(1, 3, 3)
        plt.plot(callback.loss_values, label='Loss')
        plt.xlabel('Training Step')
        plt.ylabel('Loss')
        plt.title('Loss Evolution')
        plt.legend()

        plt.tight_layout()
        plt.show()

        plt.savefig(f'training plots{lr}{n_step}.png')  
        plt.close()