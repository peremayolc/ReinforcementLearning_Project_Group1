# -*- coding: utf-8 -*-
"""PRA2_LunarLander (2 extension).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Py7uMGdgSuicUY3qk4vCJ-CTKDa0Qva9

# Practical Activity 2 (**PRA2**)

## Evaluable Practical Exercise

<u>General considerations</u>:

- The proposed solution cannot use methods, functions or parameters declared **_deprecated_** in future versions.
- This activity must be carried out on a **strictly individual** basis. Any indication of copying will be penalized with a failure for all parties involved and the possible negative evaluation of the subject in its entirety.
- It is necessary for the student to indicate **all the sources** that she/he has used to carry out the PRA. If not, the student will be considered to have committed plagiarism, being penalized with a failure and the possible negative evaluation of the subject in its entirety.

<u>Delivery format</u>:

- Some exercises may require several minutes of execution, so the delivery must be done in **Notebook format** and in **HTML format**, where the code, results and comments of each exercise can be seen. You can export the notebook to HTML from the menu File $\to$ Download as $\to$ HTML.
- There is a special type of cell to hold text. This type of cell will be very useful to answer the different theoretical questions posed throughout the activity. To change the cell type to this type, in the menu: Cell $\to$ Cell Type $\to$ Markdown.

<div class="alert alert-block alert-info">
<strong>Andreu Gascón Marzo (1670919)</strong>
</div>

## Introduction

[Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment is part of the [Box2D](https://gymnasium.farama.org/environments/box2d/) environments. Lunar Lander is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.

There are two versions of the environment: discrete and continuous. The landing pad is always at the coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.

<img src="https://gymnasium.farama.org/_images/lunar_lander.gif" width="400px" />

More information can be found at:
- [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/)

In order to initialize the environment, we will use the discrete action space `continuous=False`, as depicted in the following code:

<u>Notes</u>: The Lunar Lander environment requires the `box2d-py` library:
- Local environment:

> conda install swig

> pip install box2d-py

- Google Colab, always start notebook with:

> !pip install box2d-py
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import gymnasium as gym
from IPython.display import display, Image as IPImage
import time
import numpy as np
import collections
import wandb
import torch.optim as optim
from torchsummary import summary
from itertools import product
import os
import json

import gymnasium as gym

import ale_py
from ale_py import ALEInterface
from gymnasium.wrappers import TransformReward

#set the environment as Kaboom
env = gym.make("ALE/Kaboom-v5", obs_type="grayscale")


"""## Part 1. Deep Q-Networks

In the in-class examples and exercises, we have designed and implemented a Deep Q-Network (DQN) incorporating an experience replay buffer, target network, and the $\epsilon$-greedy exploration strategy.

For this exercise, you can reuse and adapt this code to meet your specific needs. Alternatively, you are encouraged to develop your own code or use third-party code examples (with appropriate citations in your activity submission).

<u>Questions</u> (**6 points**):

1. Implement a Baseline DQN: Develop a DQN model that includes an experience replay buffer, target network, and $\epsilon$-greedy method as a baseline.

2. Train and Tune: Train the baseline model and adjust its hyperparameters to optimize performance.

3. Report Metrics: Provide key metrics for the baseline model, such as rolling average reward, loss values, etc., similar to previous exercises.

4. Extensions (Choose 2 or more): Select and implement two (or more) of the following DQN extensions in your model:

- Prioritized Experience Replay Buffer
- N-step DQN (N=2)
- Double DQN
- Dueling DQN

5. For each extension:

- Add comments to the code to highlight the modifications made.
- Train the extended model and fine-tune its hyperparameters.
- Report the performance metrics and compare them to the baseline model.

<div class="alert alert-block alert-danger">
<strong>Solution</strong>
</div>

<p style="color: orange; font-weight: bold; font-size: 24px;">Basic Agent Training</p>
"""

# OpenAI Gym Wrappers
# Taken from 
# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py
import cv2
import numpy as np
import collections
import gym.spaces

# chekcs for compatibility of the step return                
class EnvCompatibility(gym.Wrapper):
    def step(self, action):
        results = self.env.step(action)
        if len(results) == 4:
            observation, reward, done, info = results
            # Since the base env doesn't return truncated, assume truncated is False
            return observation, reward, done, False, info
        elif len(results) == 5:
            return results
        else:
            raise ValueError("Unexpected number of values returned from env.step()")

    def reset(self, **kwargs):
        results = self.env.reset(**kwargs)
        if isinstance(results, tuple) and len(results) == 2:
            return results
        else:
            # Assume info is an empty dict if not provided
            return results, {}

class FireResetEnv(gym.Wrapper):
    def reset(self):
        self.env.reset()
        # First action
        results = self.env.step(1)
        if len(results) == 5:
            obs, _, term, trunc, info = results
            done = term or trunc
        elif len(results) == 4:
            obs, _, done, info = results
        else:
            raise ValueError("Unexpected number of return values from env.step()")
        if done:
            self.env.reset()
        # Second action
        results = self.env.step(2)
        if len(results) == 5:
            obs, _, term, trunc, info = results
            done = term or trunc
        elif len(results) == 4:
            obs, _, done, info = results
        else:
            raise ValueError("Unexpected number of return values from env.step()")
        if done:
            self.env.reset()
        return obs, info

    
class MaxAndSkipEnv(gym.Wrapper):
    def __init__(self, env=None, skip=4):
        super(MaxAndSkipEnv, self).__init__(env)
        self.obs_buffer = collections.deque(maxlen=2)  # Changed from _obs_buffer to obs_buffer
        self.skip = skip  # Changed from _skip to skip

    def step(self, action):
        total_reward = 0.0
        terminated = False
        truncated = False
        info = {}
        for _ in range(self.skip):  # Accessing self.skip instead of self._skip
            results = self.env.step(action)
            if len(results) == 5:
                obs, reward, term, trunc, info = results
                terminated = terminated or term
                truncated = truncated or trunc
            elif len(results) == 4:
                obs, reward, done, info = results
                terminated = done
                truncated = False
            else:
                raise ValueError("Unexpected number of return values from env.step()")
            self.obs_buffer.append(obs)
            total_reward += reward
            if terminated or truncated:
                break
        max_frame = np.max(np.stack(self.obs_buffer), axis=0)
        return max_frame, total_reward, terminated, truncated, info

class ProcessFrame84(gym.ObservationWrapper):
    def __init__(self, env=None):
        super(ProcessFrame84, self).__init__(env)
        self.observation_space = gym.spaces.Box(
            low=0, high=255, shape=(84, 84), dtype=np.uint8
        )

    def observation(self, obs):
        return self.process(obs)

    @staticmethod
    def process(frame):
        # Resize and crop the frame
        resized_screen = cv2.resize(frame, (84, 110), interpolation=cv2.INTER_AREA)
        cropped_frame = resized_screen[18:102, :]  # Crop to 84x84
        return cropped_frame.astype(np.uint8)


class BufferWrapper(gym.ObservationWrapper):
    def __init__(self, env, n_steps, dtype=np.float32):
        super(BufferWrapper, self).__init__(env)
        self.dtype = dtype
        self.n_steps = n_steps
        self.frames = collections.deque(maxlen=n_steps)
        old_space = env.observation_space

        # Update the observation space to reflect the stacked frames
        self.observation_space = gym.spaces.Box(
            low=0.0,
            high=1.0,
            shape=(n_steps, old_space.shape[0], old_space.shape[1]),
            dtype=dtype,
        )
    def reset(self):
        self.frames.clear()
        obs, info = self.env.reset()
        for _ in range(self.n_steps):
            self.frames.append(np.zeros_like(obs))
        self.frames.append(obs)
        return self.observation(obs), info

    def observation(self, observation):
        self.frames.append(observation)
        # Stack frames along the first dimension (channels)
        return np.array(self.frames, dtype=self.dtype)


class ScaledFloatFrame(gym.ObservationWrapper):
    def observation(self, obs):
        return np.array(obs).astype(np.float32) / 255.0


import cv2
import numpy as np
import collections
import gym
from gym import spaces

import cv2
import numpy as np
import gym
from gym import spaces

class EnhancedRewardWrapper(gym.Wrapper):
    def __init__(self, env, initial_lives=3, penalty=-5, bonus=1, 
                 level_ascend_reward=10, level_descend_penalty=-10, 
                 level_threshold=1000):
        """
        Enhanced Reward Wrapper that detects explosions, counts lives, and monitors level changes.

        Args:
            env (gym.Env): The environment to wrap.
            initial_lives (int): Starting number of lives.
            penalty (float): Penalty for an explosion.
            bonus (float): Bonus for maintaining lives or desired behaviors.
            level_ascend_reward (float): Reward for ascending a level.
            level_descend_penalty (float): Penalty for descending a level.
            level_threshold (int): Score threshold to determine level changes.
        """
        super(EnhancedRewardWrapper, self).__init__(env)
        self.initial_lives = initial_lives
        self.lives = initial_lives
        self.penalty = penalty
        self.bonus = bonus
        self.level_ascend_reward = level_ascend_reward
        self.level_descend_penalty = level_descend_penalty
        self.level_threshold = level_threshold
        self.previous_frame = None
        self.explosion_detected = False
        self.current_level = 1
        self.previous_level = 1
        self.previous_score = 0

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.lives = self.initial_lives
        self.previous_frame = self.preprocess_frame(obs)
        self.previous_score = self.extract_score(info)
        self.previous_level = self.calculate_level(self.previous_score)
        self.current_level = self.previous_level
        return obs, info

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        # Initialize reward components
        total_reward = 0.0
        explosion_penalty_applied = False
        level_reward_applied = 0.0
        survival_bonus_applied = False

        # Preprocess current frame
        current_frame = self.preprocess_frame(obs)
        
        # Detect explosion
        explosion = self.detect_explosion(self.previous_frame, current_frame)
        
        if explosion:
            self.lives -= 1
            total_reward += self.penalty  # Penalize for explosion
            explosion_penalty_applied = True
            print(f"Explosion detected! Lives remaining: {self.lives}")
            if self.lives <= 0:
                terminated = True
                print("No lives left. Episode terminated.")
        else:
            # Provide a small bonus for surviving the step
            total_reward += self.bonus
            survival_bonus_applied = True
        
        # Detect level changes
        current_score = self.extract_score(info)
        current_level = self.calculate_level(current_score)
        
        if current_level > self.current_level:
            # Level Ascended
            level_reward = self.level_ascend_reward
            total_reward += level_reward
            level_reward_applied = level_reward
            print(f"Level Up! Ascended to Level {current_level}. Reward: {level_reward}")
        elif current_level < self.current_level:
            # Level Descended
            level_penalty = self.level_descend_penalty
            total_reward += level_penalty
            level_reward_applied = level_penalty
            print(f"Level Down! Descended to Level {current_level}. Penalty: {level_penalty}")
        
        # Update current level
        self.current_level = current_level
        
        # Update previous frame
        self.previous_frame = current_frame
        
        # Update previous score
        self.previous_score = current_score
        
        # Add lives and level information to info
        info['lives'] = self.lives
        info['current_level'] = self.current_level
        info['score'] = current_score

        # Log reward components
        if explosion_penalty_applied:
            print(f"Applied Explosion Penalty: {self.penalty}")
        if survival_bonus_applied:
            print(f"Applied Survival Bonus: {self.bonus}")
        if level_reward_applied != 0.0:
            print(f"Applied Level Reward: {level_reward_applied}")
        
        # Sum the environment's original reward with the wrapper's modifications
        modified_reward = reward + total_reward

        return obs, modified_reward, terminated, truncated, info

    def preprocess_frame(self, frame):
        """
        Preprocess the frame for explosion detection.
        Convert to grayscale and resize if necessary.
        """
        # Ensure frame has 3 channels (RGB)
        if len(frame.shape) == 3 and frame.shape[2] == 3:
            gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        elif len(frame.shape) == 2:
            # Frame is already grayscale
            gray = frame
        else:
            raise ValueError(f"Unexpected frame shape: {frame.shape}")
        
        resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)
        return resized

    def detect_explosion(self, prev_frame, current_frame, threshold=50):
        """
        Detects an explosion by measuring the change between frames.
        
        Args:
            prev_frame (np.array): The previous grayscale frame.
            current_frame (np.array): The current grayscale frame.
            threshold (float): Threshold for detecting significant changes.
        
        Returns:
            bool: True if an explosion is detected, False otherwise.
        """
        if prev_frame is None:
            return False

        # Compute absolute difference between frames
        diff = cv2.absdiff(current_frame, prev_frame)
        mean_diff = np.mean(diff)
        
        # Debug: Print mean difference
        # print(f"Mean frame difference: {mean_diff}")

        # If the mean difference exceeds the threshold, assume an explosion
        return mean_diff > threshold

    def extract_score(self, info):
        """
        Extracts the current score from the info dictionary.
        
        Args:
            info (dict): Information dictionary from the environment.
        
        Returns:
            int: Current score.
        """
        # Log the info dictionary for debugging
        print(f"Info Dictionary: {info}")

        # Adjust based on actual info structure
        if 'score' in info:
            return info['score']
        elif 'total_reward' in info:
            return int(info['total_reward'])
        else:
            # If no score is available, track manually or adjust accordingly
            return self.previous_score

    def calculate_level(self, score):
        """
        Calculates the current level based on the score.
        
        Args:
            score (int): Current score.
        
        Returns:
            int: Current level.
        """
        # Define how levels are determined from the score.
        # For example, each level requires an additional 'level_threshold' points.
        return max(1, score // self.level_threshold + 1)







def make_env(env):
    print("Standard Env.        : {}".format(env.observation_space.shape))
    env = MaxAndSkipEnv(env)
    print("MaxAndSkipEnv        : {}".format(env.observation_space.shape))
    env = FireResetEnv(env)
    print("FireResetEnv         : {}".format(env.observation_space.shape))
    env = ProcessFrame84(env)
    print("ProcessFrame84       : {}".format(env.observation_space.shape))
    
    # Insert EnhancedRewardWrapper before BufferWrapper
    env = EnhancedRewardWrapper(
        env, 
        initial_lives=3, 
        penalty=-5, 
        bonus=0, 
        level_ascend_reward=10, 
        level_descend_penalty=-10, 
        level_threshold=1000  # Adjust based on game mechanics
    )
    print("EnhancedRewardWrapper: Applied")
    
    env = BufferWrapper(env, 4)
    print("BufferWrapper        : {}".format(env.observation_space.shape))
    env = ScaledFloatFrame(env)
    print("ScaledFloatFrame     : {}".format(env.observation_space.shape))
    env = EnvCompatibility(env)
    
    return env



def print_env_info(name, env):
    obs,info = env.reset()
    print("*** {} Environment ***".format(name))
    print("Observation shape: {}, type: {} and range [{},{}]".format(obs.shape, obs.dtype, np.min(obs), np.max(obs)))
    print("Observation sample:\n{}".format(obs))


'________________________________________________________________________________________________________-'



if torch.cuda.is_available():
    device = torch.device("cuda")
    print('cuda')
else:
    device = torch.device("cpu")

import torch
import torch.nn as nn
'____________________________________________________________________________________________________________________________________________________________'
class DQN(nn.Module):
    def __init__(self, input_shape, action_dim):
        """
        Initializes a convolutional DQN for processing visual input.
        Args:
            input_shape (tuple): The shape of the input observation (e.g., (4, 84, 84)).
            action_dim (int): The number of possible actions in the environment.
        """
        super(DQN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)

        # Fully connected layers
        # Calculate the flattened size after the convolutional layers
        conv_output_size = self._get_conv_output_size(input_shape)
        self.fc1 = nn.Linear(conv_output_size, 512)
        self.fc2 = nn.Linear(512, action_dim)

        # Apply Xavier initialization
        self._initialize_weights()

    def _get_conv_output_size(self, input_shape):
        """
        Computes the size of the output after passing the input through the convolutional layers.
        Args:
            input_shape (tuple): The shape of the input observation (e.g., (4, 84, 84)).
        Returns:
            int: Flattened size after convolutions.
        """
        # Create a dummy tensor to pass through the convolutional layers
        with torch.no_grad():
            dummy_input = torch.zeros(1, *input_shape)  # Batch size of 1
            x = self.conv1(dummy_input)
            x = self.conv2(x)
            x = self.conv3(x)
        return x.numel()  # Total number of elements in the tensor

    def _initialize_weights(self):
        """
        Initializes weights of the network using Xavier initialization.
        """
        for layer in self.modules():
            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):
                torch.nn.init.xavier_uniform_(layer.weight)
                if layer.bias is not None:
                    layer.bias.data.fill_(0.01)

    def forward(self, x):
        """
        Forward pass of the DQN.
        Args:
            x (torch.Tensor): Input tensor representing the state.
        Returns:
            torch.Tensor: Q-values for each action.
        """
        # Pass through convolutional layers
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.relu(self.conv3(x))

        # Flatten and pass through fully connected layers
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)  # Output Q-values
        return x

'____________________________________________________________________________________________________________________________________________________________'
"""**REPLAY** **BUFFER**"""

class SumTree:
    def __init__(self, capacity):
        """
        Initialize the SumTree with a given capacity.
        """
        self.capacity = capacity  # Number of leaf nodes (max number of experiences)
        self.tree = np.zeros(2 * capacity - 1)  # Total number of nodes in the tree
        self.data = np.zeros(capacity, dtype=object)  # Stores the actual experiences
        self.write = 0  # Pointer to where the next experience will be written
        self.n_entries = 0  # Total number of experiences stored

    def _propagate(self, idx, change):
        """
        Update the tree by propagating the change up to the root.
        """
        parent = (idx - 1) // 2
        self.tree[parent] += change

        if parent != 0:
            self._propagate(parent, change)

    def update(self, idx, priority):
        """
        Update the priority of a given leaf node and propagate the change.
        """
        change = priority - self.tree[idx]
        self.tree[idx] = priority
        self._propagate(idx, change)

    def add(self, priority, data):
        """
        Add a new experience with its priority to the tree.
        """
        idx = self.write + self.capacity - 1  # Calculate the leaf index
        self.data[self.write] = data  # Store the experience
        self.update(idx, priority)  # Update the tree with the new priority

        self.write += 1  # Move the write pointer
        if self.write >= self.capacity:
            self.write = 0  # Overwrite if capacity is exceeded

        if self.n_entries < self.capacity:
            self.n_entries += 1

    def _retrieve(self, idx, s):
        """
        Find the leaf node that corresponds to the cumulative sum s.
        """
        left = 2 * idx + 1
        right = left + 1

        if left >= len(self.tree):
            return idx

        if s <= self.tree[left]:
            return self._retrieve(left, s)
        else:
            return self._retrieve(right, s - self.tree[left])

    def get(self, s):
        """
        Get the experience corresponding to the cumulative sum s.
        """
        idx = self._retrieve(0, s)
        dataIdx = idx - self.capacity + 1
        return (idx, self.tree[idx], self.data[dataIdx])

    @property
    def total(self):
        """
        Get the total priority.
        """
        return self.tree[0]
'____________________________________________________________________________________________________________________________________________________________'
Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state','td_error'])
class ExperienceReplay:
    def __init__(self, capacity, alpha=0.6, epsilon=1e-5):
        self.tree = SumTree(capacity)
        self.alpha = alpha
        self.epsilon = epsilon

    def append(self, experience, td_error=1.0):
        priority = (abs(td_error) + self.epsilon) ** self.alpha
        self.tree.add(priority, experience)


    def sample(self, batch_size, beta=0.4):
        self.tree.total == 0

        batch = []
        idxs = []
        segment = self.tree.total / batch_size
        priorities = []

        for i in range(batch_size):
            a = segment * i
            b = segment * (i + 1)
            s = np.random.uniform(a, b)
            idx, priority, data = self.tree.get(s)
            batch.append(data)
            idxs.append(idx)
            priorities.append(priority)

        # Add a small constant to avoid division by zero
        sampling_probabilities = priorities / (self.tree.total + 1e-8)
        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -beta)
        is_weight /= is_weight.max()
        is_weight = np.array(is_weight, dtype=np.float32)

        # Unpack experiences
        states, actions, rewards, dones, next_states, _ = zip(*batch)

        return (
            np.array(states),
            np.array(actions),
            np.array(rewards, dtype=np.float32),
            np.array(dones, dtype=np.uint8),
            np.array(next_states),
            is_weight,
            idxs
        )

    def __len__(self):
        return self.tree.n_entries

    def update_priorities(self, idxs, td_errors):
        for idx, td_error in zip(idxs, td_errors):
            priority = (abs(td_error) + self.epsilon) ** self.alpha
            self.tree.update(idx, priority)

"""**DEEP Q-LEARNING ALGORITHM**"""
'____________________________________________________________________________________________________________________________________________________________'
Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state','td_error'])


class Agent:
    def __init__(self, env, exp_replay_buffer):
        """
        Initialize the agent.

        Args:
            env: The environment the agent interacts with (e.g., OpenAI Gym environment).
            exp_replay_buffer: ExperienceReplay buffer to store experiences.
        """

        self.env = env  # Reference to the environment

        self.exp_replay_buffer = exp_replay_buffer  # Replay buffer for storing experiences

        self._reset()  # Reset agent state and rewards


    def _reset(self):
        """
        Resets the agent's current state by resetting the environment.
        """

        self.current_state, info = self.env.reset()  # Unpack the tuple

        self.total_reward = 0.0

    def step(self, net, epsilon=0.0, device="cpu"):
        """
        Perform a single step in the environment, choosing an action based on an epsilon-greedy policy.

        Args:
            net: The DQN network for predicting Q-values.
            epsilon: Probability of selecting a random action (for exploration).
            device: Device to perform computations on (e.g., "cpu" or "cuda").

        Returns:
            done_reward (float or None): Total reward if the episode ends; otherwise, None.
        """

        done_reward = None  # Initialize reward for the end of the episode

        if np.random.random() < epsilon:  # With probability epsilon, take a random action

            action = self.env.action_space.sample()  # Select random action for exploration

        else:
            state_ = np.array([self.current_state])  # Prepare state as a batch (1, state_dim)

            state = torch.tensor(state_).to(device)  # Convert to tensor and send to device

            q_vals = net(state)  # Get Q-values for the current state

            _, act_ = torch.max(q_vals, dim=1)  # Select the action with the highest Q-value

            action = int(act_.item())  # Convert action from tensor to integer


        # Execute the action in the environment

        try:
            new_state, reward, terminated, truncated, info = self.env.step(action)
            #print(f"Lives: {info['custom_lives']} | Reward: {reward}")
            is_done = terminated or truncated
        except ValueError:
            # Fallback for environments returning 4 values
            new_state, reward, done, info = self.env.step(action)
            is_done = done
            terminated = done
            truncated = False
            info = info
            print(f"Lives: {info['custom_lives']} | Reward: {reward}")
            print(new_state)
        self.total_reward += reward
        exp = Experience(self.current_state, action, reward, is_done, new_state, td_error=1.0)
        self.exp_replay_buffer.append(exp)
        self.current_state = new_state
        if is_done:
            done_reward = self.total_reward
            self._reset()
        return reward, done_reward

'____________________________________________________________________________________________________________________________________________________________'
"""**MAIN BUCLE**"""

def train_model(env, hyperparameters, target_reward):
    # Unpack hyperparameters
    learning_rate = hyperparameters["learning_rate"]
    batch_size = hyperparameters["batch_size"]
    epsilon_decay = hyperparameters["epsilon_decay"]
    gamma = hyperparameters["gamma"]

    # Initialize wandb for this run
    wandb.init(
        project="DQN_Kaboom",
        config={
            "learning_rate": learning_rate,
            "batch_size": batch_size,
            "epsilon_decay": epsilon_decay,
            "gamma": gamma,
            "target_reward": target_reward,
            "experience_replay_size": EXPERIENCE_REPLAY_SIZE,
            "sync_target_network": SYNC_TARGET_NETWORK,
            "eps_start": EPS_START,
            "eps_min": EPS_MIN,
            "number_of_rewards_to_average": NUMBER_OF_REWARDS_TO_AVERAGE,
        }
    )

    net = DQN(env.observation_space.shape, env.action_space.n).to(device)
    target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)
    target_net.load_state_dict(net.state_dict())
    target_net.eval()

    buffer = ExperienceReplay(EXPERIENCE_REPLAY_SIZE)
    agent = Agent(env, buffer)

    epsilon = EPS_START
    optimizer = optim.Adam(net.parameters(), lr=learning_rate)
    total_rewards = []
    frame_number = 0
    loss_history = []
    avg_reward_history = []

    done = False
    # Optional: Watch the model
    wandb.watch(net, log="all", log_freq=10)

    while len(total_rewards) == 500:
        frame_number += 1
        epsilon = max(epsilon * epsilon_decay, EPS_MIN)
        
        # Get immediate and cumulative rewards
        immediate_reward, cumulative_reward = agent.step(net, epsilon=epsilon, device=device)

        print(f"Step: {frame_number} | Immediate Reward: {immediate_reward}")

        # Log immediate reward to wandb
        wandb.log({
            "immediate_reward": immediate_reward,
            "frame_number": frame_number,
        })

        # If an episode ends, log cumulative reward
        if cumulative_reward is not None:
            print('Episode Ended with Cumulative Reward:', cumulative_reward)
            total_rewards.append(cumulative_reward)

            mean_reward = np.mean(total_rewards[-NUMBER_OF_REWARDS_TO_AVERAGE:])

            wandb.log({
                "cumulative_reward": cumulative_reward,
                "episode": len(total_rewards),
                "mean_reward": mean_reward,
                "epsilon": epsilon,
                "frame_number": frame_number,
            })

            if len(total_rewards) % 10 == 0 or mean_reward > target_reward:
                print(f"Frame:{frame_number} | Total games:{len(total_rewards)} | Mean reward: {mean_reward:.3f} (epsilon: {epsilon:.2f})")

            avg_reward_history.append(mean_reward)

            if mean_reward > target_reward:
                #print(f"SOLVED in {frame_number} frames and {len(total_rewards)} games")
                break

        if len(buffer) < batch_size:
            continue

        # Sample a batch
        try:
            states, actions, rewards, dones, next_states, is_weights, indices = buffer.sample(batch_size)
        except ValueError as e:
            print(f"Sampling error: {e}")
            continue

        # Convert to tensors
        states = torch.tensor(states, dtype=torch.float32).to(device)
        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)
        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(device)
        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)
        dones = torch.BoolTensor(dones).unsqueeze(1).to(device)
        is_weights = torch.tensor(is_weights, dtype=torch.float32).unsqueeze(1).to(device)

        # Compute current Q values
        Q_values = net(states).gather(1, actions)

        with torch.no_grad():
            # Compute next Q values from target network
            next_Q_values = target_net(next_states).max(1, keepdim=True)[0]
            next_Q_values[dones] = 0.0

        # Compute expected Q values
        expected_Q_values = rewards + (gamma * next_Q_values)

        # Compute TD errors
        td_errors = Q_values - expected_Q_values

        # Compute loss with importance-sampling weights
        loss = (td_errors.pow(2) * is_weights).mean()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        loss_history.append(loss.item())

        # Log loss to wandb
        wandb.log({
            "loss": loss.item(),
            "frame_number": frame_number,
        })

        # Update priorities in the buffer
        buffer.update_priorities(indices, td_errors.detach().cpu().numpy().flatten())

        # Synchronize target network
        if frame_number % SYNC_TARGET_NETWORK == 0:
            target_net.load_state_dict(net.state_dict())

    # Finish the wandb run
    wandb.finish()

    return net, np.mean(total_rewards[-100:]), loss_history, avg_reward_history

'____________________________________________________________________________________________________________________________________________________________'

def plot_results(hyperparameters, loss_history, avg_reward_history, mean_reward, save_dir, smoothing_window=10):
    # Helper function to compute moving average
    def moving_average(data, window_size):  
        return np.convolve(data, np.ones(window_size) / window_size, mode='valid')

    hyperparameter_str = "_".join([f"{k}-{v}".replace(" ", "_") for k, v in hyperparameters.items()])

    fig, axes = plt.subplots(2, 1, figsize=(8, 12))

    # Plot Loss Evolution with smoothing
    smoothed_loss = moving_average(loss_history, smoothing_window)
    axes[0].plot(loss_history, label="Loss", alpha=0.5, color='gray')  # Original loss (lighter for comparison)
    axes[0].plot(range(len(smoothed_loss)), smoothed_loss, label=f"Smoothed Loss (window={smoothing_window})", color='blue')  # Smoothed loss
    axes[0].set_title(f"Loss Evolution")
    axes[0].set_xlabel("Iterations")
    axes[0].set_ylabel("Loss")
    axes[0].legend()

    # Plot Average Reward Evolution
    axes[1].plot(avg_reward_history, label="Average Reward")
    axes[1].set_title(f"Average Reward Evolution")
    axes[1].set_xlabel("Iterations")
    axes[1].set_ylabel("Average Reward")
    axes[1].legend()

    # Adjust layout to avoid overlap
    plt.tight_layout()

    # Save the combined plot
    plt.savefig(os.path.join(save_dir, f"combined_plots_{hyperparameter_str}.png"))
    plt.close()

def save_training_data(save_dir, hyperparameters, avg_reward_history, loss_history):
    # Prepare data to save
    data = {
        "hyperparameters": hyperparameters,
        "average_reward_history": avg_reward_history,
        "loss_history": loss_history
    }

    # Define the file path
    file_path = os.path.join(save_dir, "training_data.json")

    # Write data to a JSON file
    with open(file_path, 'w') as f:
        json.dump(data, f, indent=4)
    print("Training data saved to", file_path)
'____________________________________________________________________________________________________________________________________________________________'

def watch_agent_reinforce(env, models_dir, num_episodes=100, max_experiments=8):
    """
    Evaluates the first 'max_experiments' REINFORCE models in the specified directory and its subdirectories over a number of episodes,
    selects the best-performing model based on average cumulative reward,
    and returns the rewards and action lists for the best model.
    
    Args:
        env (gym.Env): The Gymnasium environment.
        models_dir (str): Directory containing subdirectories of REINFORCE model files (.pth).
        num_episodes (int): Number of episodes to run for each model.
        max_experiments (int): Maximum number of experiments to evaluate.
    
    Returns:
        best_model (nn.Module): The best-performing REINFORCE model.
        best_avg_reward (float): Average cumulative reward of the best model.
        best_rewards (list): List of cumulative rewards for each episode of the best model.
        best_actions (list): List of action sequences for each episode of the best model.
    """
    # Initialize variables to track the best model
    best_avg_reward = -float('inf')
    best_model = None
    best_rewards = []
    best_actions = []
    best_model_name = ""
    
    # Get a sorted list of subdirectories to ensure consistent ordering
    sorted_subdirs = sorted([d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))])
    
    # Limit to the first 'max_experiments' subdirectories
    selected_subdirs = sorted_subdirs[:max_experiments]
    
    for subdir in selected_subdirs:
        subdir_path = os.path.join(models_dir, subdir)
        model_path = os.path.join(subdir_path, "model.pth")
        
        if os.path.exists(model_path):
            print(f"Evaluating model: {model_path}")
            
            # Initialize the model (REINFORCE uses the same architecture as DQN without softmax)
            model = DQN(env.observation_space.shape[0], env.action_space.n).to(device)
            
            try:
                # Load the model weights
                model.load_state_dict(torch.load(model_path, map_location=device))
            except Exception as e:
                print(f"Error loading {model_path}: {e}")
                continue
      
            # Set the model to evaluation mode
            model.eval()
            
            # Lists to store rewards and actions for this model
            rewards = []
            actions = []
            
            for episode in range(num_episodes):
                state, _ = env.reset()
                episode_reward = 0
                episode_actions = []
                done = False
                
                while not done:
                    # Convert state to tensor
                    state_tensor = torch.tensor(state, dtype=torch.float32).to(device)
                    
                    # Get action probabilities from the model (logits are converted via softmax)
                    with torch.no_grad():
                        logits = model(state_tensor)
                        action_probs = torch.softmax(logits, dim=-1).cpu().numpy()
                    
                    # Sample an action based on the probabilities
                    action = np.random.choice(env.action_space.n, p=action_probs)
                    episode_actions.append(action)
                    
                    # Take the action in the environment
                    next_state, reward, terminated, truncated, _ = env.step(action)
                    done = terminated or truncated
                    episode_reward += reward
                    state = next_state

        
                rewards.append(episode_reward)
                actions.append(episode_actions)
            
            # Calculate the average reward for this model
            avg_reward = np.mean(rewards)
            print(f"Average Reward for {model_path}: {avg_reward:.2f}")
            
            # Update the best model if this model has a higher average reward
            if avg_reward > best_avg_reward:
                best_avg_reward = avg_reward
                best_model = model
                best_rewards = rewards
                best_actions = actions
                best_model_name = subdir
    
    if best_model is not None:
        print(f"\nBest Model: {best_model_name} with Average Reward: {best_avg_reward:.2f}")
    else:
        print("No valid models were evaluated.")
    
    return best_model, best_avg_reward, best_rewards, best_actions
'____________________________________________________________________________________________________________________________________________________________'

def hyperparameter_tuning(env, hyperparameter_space, target_reward, save_dir):
    # Create directory to save models
    os.makedirs(save_dir, exist_ok=True)

    best_model = None
    best_score = -float("inf")
    best_hyperparameters = None
    best_experiment_index = None

    # Generate all combinations of hyperparameters
    keys = list(hyperparameter_space.keys())  # Extract hyperparameter names
    values = list(hyperparameter_space.values())  # Extract hyperparameter values

    # Use itertools.product to create all combinations of hyperparameter values
    combinations = product(*values)  # Returns tuples of combinations

    # Convert each combination into a dictionary with corresponding hyperparameter names
    hyperparameter_combinations = []
    for combination in combinations:
        hyperparameter_dict = dict(zip(keys, combination))
        hyperparameter_combinations.append(hyperparameter_dict)


    for i, hyperparameters in enumerate(hyperparameter_combinations):
        experiment_dir = os.path.join(save_dir, f"experiment_{i + 1}")
        os.makedirs(experiment_dir, exist_ok=True)

        print(f"Experiment {i + 1}/{len(hyperparameter_combinations)}: {hyperparameters}")
        model, mean_reward, loss_history, avg_reward_history = train_model(env, hyperparameters, target_reward)

        print(f"Mean Reward: {mean_reward:.2f}")
        #plot_results(hyperparameters, loss_history, avg_reward_history, mean_reward, experiment_dir)

        model_path = os.path.join(experiment_dir, "model.pth")
        torch.save(model.state_dict(), model_path)
        print(f"New model saved with reward {mean_reward:.2f}")

        hyperparameters_save_path = os.path.join(experiment_dir, "hyperparameters.json")
        with open(hyperparameters_save_path, "w") as f:
            json.dump(hyperparameters, f, indent=4)
        print(f"Hyperparameters saved: {hyperparameters}")

        save_training_data(experiment_dir, hyperparameters, avg_reward_history, loss_history)

        # Save the model if it's the best so far
        if mean_reward > best_score:
            best_score = mean_reward
            best_model = model
            best_hyperparameters = hyperparameters

            best_loss_history = loss_history
            best_avg_reward_history = avg_reward_history
            best_experiment_index = i + 1

    best_model_info_path = os.path.join(save_dir, "best_model_info.txt")
    with open(best_model_info_path, "w") as f:
        f.write(f"Best model is experiment {best_experiment_index}\n")
        f.write(f"Best hyperparameters: {best_hyperparameters}\n")
        f.write(f"Best mean reward: {best_score:.2f}\n")
    print(f"Best model information saved at: {best_model_info_path}")
    print(f"Best Hyperparameters: {best_hyperparameters}")
    print(f"Best Mean Reward: {best_score:.2f}")

    return best_model, best_hyperparameters, best_loss_history, best_avg_reward_history

hyperparameter_space = {
    "learning_rate": [0.0005, 0.001],
    "batch_size": [64, 128],
    "epsilon_decay": [0.999985, 0.99993],
    "gamma": [0.99],
}





NUMBER_OF_REWARDS_TO_AVERAGE = 10

EXPERIENCE_REPLAY_SIZE = 10000

SYNC_TARGET_NETWORK = 1000 # syncronize target neuron after 1000 steps

# greedy policy
EPS_START = 1.0
EPS_MIN = 0.02

target_reward=200

DQN_dir = "C:/GitHub Repositories/ReinforcementLearning_Project_Group1/Andreu/Working"

print(gym.__version__)

test_env = make_env(env)


best_DQN_model, best_hyperparameters, best_DQN_loss_history, best_DQN_avg_reward_history = hyperparameter_tuning(
    env=test_env,
    hyperparameter_space=hyperparameter_space,  # Maximum number of frames
    target_reward=target_reward,# Reward threshold to consider the task solved
    save_dir=DQN_dir,  # Directory to save the best model
)